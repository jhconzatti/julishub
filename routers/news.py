from fastapi import APIRouter, HTTPException
import feedparser
import requests
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import logging
from hashlib import md5

router = APIRouter()

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cache simples em mem√≥ria (1 hora)
_cache_news = {
    "data": None,
    "timestamp": None,
}

CACHE_DURATION = timedelta(hours=1)


def is_cache_valid(cache_timestamp: Optional[datetime]) -> bool:
    """Verifica se o cache ainda √© v√°lido (menos de 1 hora)"""
    if cache_timestamp is None:
        return False
    return datetime.now() - cache_timestamp < CACHE_DURATION


def get_fonte_display_name(link: str) -> str:
    """Extrai o nome amig√°vel da fonte a partir do link"""
    fonte_map = {
        "infomoney": "InfoMoney",
        "g1.globo": "G1",
        "valor": "Valor Econ√¥mico",
        "exame": "Exame",
        "estadao": "Estad√£o",
        "folha": "Folha de S.Paulo",
        "uol": "UOL Economia",
        "cnnbrasil": "CNN Brasil",
        "moneytimes": "Money Times",
        "seudinheiro": "Seu Dinheiro",
        "investnews": "InvestNews",
        "neofeed": "NeoFeed",
        "investidor10": "Investidor10",
        "suno": "Suno Not√≠cias",
    }
    
    link_lower = link.lower()
    for key, name in fonte_map.items():
        if key in link_lower:
            return name
    
    # Fallback: tenta extrair dom√≠nio
    try:
        from urllib.parse import urlparse
        domain = urlparse(link).netloc
        # Remove www. e .com.br/.com
        domain = domain.replace("www.", "").replace(".com.br", "").replace(".com", "")
        return domain.capitalize()
    except:
        return "Fonte Desconhecida"


def get_placeholder_image(fonte: str) -> str:
    """Retorna imagem placeholder baseada na fonte"""
    # Gera um hash da fonte para cor consistente
    fonte_hash = int(md5(fonte.encode()).hexdigest(), 16) % 10
    
    colors = [
        "f59e0b",  # amber
        "10b981",  # emerald
        "3b82f6",  # blue
        "8b5cf6",  # violet
        "ef4444",  # red
        "06b6d4",  # cyan
        "f97316",  # orange
        "14b8a6",  # teal
        "6366f1",  # indigo
        "ec4899",  # pink
    ]
    
    color = colors[fonte_hash]
    # Placeholder via UI Avatars ou similar
    return f"https://ui-avatars.com/api/?name={fonte[0]}&background={color}&color=fff&size=400&bold=true"


def format_relative_time(pub_date_str: str) -> str:
    """Formata data para formato relativo (ex: 'H√° 2 horas')"""
    try:
        # Parse da data (Google News usa formato RFC 822)
        from email.utils import parsedate_to_datetime
        pub_date = parsedate_to_datetime(pub_date_str)
        
        now = datetime.now(pub_date.tzinfo)
        diff = now - pub_date
        
        if diff.days > 0:
            if diff.days == 1:
                return "H√° 1 dia"
            elif diff.days < 7:
                return f"H√° {diff.days} dias"
            else:
                return pub_date.strftime("%d/%m/%Y")
        
        hours = diff.seconds // 3600
        if hours > 0:
            return f"H√° {hours}h"
        
        minutes = diff.seconds // 60
        if minutes > 0:
            return f"H√° {minutes}min"
        
        return "Agora"
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao formatar data: {e}")
        return "Data desconhecida"


def fetch_google_news() -> List[Dict[str, Any]]:
    """Busca not√≠cias do RSS do Google News (Economia Brasil)"""
    try:
        logger.info("üîÑ Buscando not√≠cias do Google News...")
        
        rss_url = "https://news.google.com/rss/search?q=economia+brasil&hl=pt-BR&gl=BR&ceid=BR:pt-419"
        
        # Parse do RSS feed
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.warning("‚ö†Ô∏è Nenhuma not√≠cia encontrada no feed")
            return []
        
        noticias = []
        for entry in feed.entries[:20]:  # Limita a 20 not√≠cias
            try:
                titulo = entry.get("title", "Sem t√≠tulo")
                link = entry.get("link", "")
                pub_date = entry.get("published", "")
                
                # Extrai fonte do link
                fonte = get_fonte_display_name(link)
                
                # Tenta extrair imagem (Google News geralmente n√£o fornece)
                imagem = None
                if hasattr(entry, "media_content") and entry.media_content:
                    imagem = entry.media_content[0].get("url")
                elif hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
                    imagem = entry.media_thumbnail[0].get("url")
                
                # Se n√£o tem imagem, usa placeholder
                if not imagem:
                    imagem = get_placeholder_image(fonte)
                
                # Formata data
                data_formatada = format_relative_time(pub_date)
                
                noticia = {
                    "titulo": titulo,
                    "link": link,
                    "fonte": fonte,
                    "data_publicacao": data_formatada,
                    "imagem": imagem,
                }
                
                noticias.append(noticia)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao processar not√≠cia individual: {e}")
                continue
        
        logger.info(f"‚úÖ {len(noticias)} not√≠cias carregadas com sucesso")
        return noticias
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar not√≠cias do Google News: {e}")
        return []


@router.get("/noticias")
async def get_noticias():
    """
    Retorna lista de not√≠cias financeiras do Google News (Economia Brasil)
    
    Cache de 1 hora para evitar sobrecarga no feed do Google
    """
    try:
        # Verifica cache
        if is_cache_valid(_cache_news["timestamp"]):
            logger.info("üì¶ Retornando not√≠cias do cache")
            return _cache_news["data"]
        
        # Busca not√≠cias frescas
        noticias = fetch_google_news()
        
        if not noticias:
            # Retorna lista vazia ao inv√©s de erro para n√£o quebrar o frontend
            logger.warning("‚ö†Ô∏è Nenhuma not√≠cia dispon√≠vel, retornando lista vazia")
            return []
        
        # Atualiza cache
        _cache_news["data"] = noticias
        _cache_news["timestamp"] = datetime.now()
        
        return noticias
    
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico no endpoint /noticias: {e}")
        # Retorna lista vazia ao inv√©s de HTTPException para n√£o quebrar o frontend
        return []
